{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fca8cf",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e86639",
   "metadata": {},
   "source": [
    "## BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf27bc0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5f54eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "# print(chr(0).__repr__)\n",
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13d135b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a3821af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2429d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'\n",
      "<class 'bytes'>\n",
      "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n",
      "13\n",
      "23\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "print(utf8_encoded)\n",
    "print(type(utf8_encoded))\n",
    "print(list(utf8_encoded))\n",
    "print(len(test_string))\n",
    "print(len(utf8_encoded))\n",
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ea99199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00'\n",
      "<class 'bytes'>\n",
      "[255, 254, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 33, 0, 32, 0, 83, 48, 147, 48, 107, 48, 97, 48, 111, 48, 33, 0]\n",
      "13\n",
      "28\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf16_encoded = test_string.encode(\"utf-16\")\n",
    "print(utf16_encoded)\n",
    "print(type(utf16_encoded))\n",
    "print(list(utf16_encoded))\n",
    "print(len(test_string))\n",
    "print(len(utf16_encoded))\n",
    "print(utf16_encoded.decode(\"utf-16\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27859698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xff\\xfe\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00S0\\x00\\x00\\x930\\x00\\x00k0\\x00\\x00a0\\x00\\x00o0\\x00\\x00!\\x00\\x00\\x00'\n",
      "<class 'bytes'>\n",
      "[255, 254, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 33, 0, 0, 0, 32, 0, 0, 0, 83, 48, 0, 0, 147, 48, 0, 0, 107, 48, 0, 0, 97, 48, 0, 0, 111, 48, 0, 0, 33, 0, 0, 0]\n",
      "13\n",
      "56\n",
      "hello! こんにちは!\n"
     ]
    }
   ],
   "source": [
    "test_string = \"hello! こんにちは!\"\n",
    "utf32_encoded = test_string.encode(\"utf-32\")\n",
    "print(utf32_encoded)\n",
    "print(type(utf32_encoded))\n",
    "print(list(utf32_encoded))\n",
    "print(len(test_string))\n",
    "print(len(utf32_encoded))\n",
    "print(utf32_encoded.decode(\"utf-32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e5547c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])\n",
    "\n",
    "print(decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788e76a0",
   "metadata": {},
   "source": [
    "This function is incorrect because it attempts to decode each byte of the input `bytestring` individually.\n",
    "\n",
    "The core problem is that **UTF-8 is a variable-length encoding**. While standard ASCII characters (like 'h', 'e', 'l', 'l', 'o') are represented by a single byte, **很多其它字符需要多个bytes**. The provided function breaks these multi-byte sequences, trying to decode each constituent byte in isolation. This fails because a single byte from a multi-byte sequence is not a valid UTF-8 character on its own.\n",
    "\n",
    "-----\n",
    "\n",
    "### Example of Incorrect Results\n",
    "\n",
    "An input byte string that represents a character outside the ASCII range will cause the function to fail. Let's use the Euro sign (`€`), which is encoded in UTF-8 by the three-byte sequence `b'\\xe2\\x82\\ac'`.\n",
    "\n",
    "**Input:**\n",
    "\n",
    "```python\n",
    "euro_bytes = \"€\".encode(\"utf-8\")  # This results in b'\\xe2\\x82\\xac'\n",
    "```\n",
    "\n",
    "When `decode_utf8_bytes_to_str_wrong(euro_bytes)` is called:\n",
    "\n",
    "1.  The loop starts with the first byte, `0xe2`.\n",
    "2.  It tries to execute `bytes([0xe2]).decode(\"utf-8\")`.\n",
    "3.  The UTF-8 decoder sees the byte `0xe2` (`11100010` in binary), which signals the start of a three-byte character. Since the other two bytes are not provided, the decoder recognizes this as an incomplete and invalid sequence.\n",
    "4.  A `UnicodeDecodeError` is raised, and the program crashes.\n",
    "\n",
    "**Correct Decoding:**\n",
    "The correct way to decode is to call the `decode` method on the entire byte string at once, allowing the decoder to properly interpret the multi-byte sequences.\n",
    "\n",
    "```python\n",
    ">>> b'\\xe2\\x82\\xac'.decode(\"utf-8\")\n",
    "'€'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b6296f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xc0 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m b \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\xc0\u001b[39;00m\u001b[38;5;130;01m\\x80\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m b\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xc0 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "b = b'\\xc0\\x80'\n",
    "b.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e7717",
   "metadata": {},
   "source": [
    "A two-byte sequence that does not decode to any Unicode character is **`b'\\xc0\\x80'`**.\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation\n",
    "\n",
    "This sequence is invalid because it is an **overlong encoding**. Here's why:\n",
    "\n",
    "1.  **UTF-8 Rules**: The UTF-8 encoding scheme has specific rules to ensure that each character has only one valid byte representation. A character must be encoded using the shortest possible number of bytes.\n",
    "\n",
    "2.  **The `C0` Start Byte**: The first byte, `0xc0` (binary `11000000`), signals the start of a two-byte sequence.\n",
    "\n",
    "3.  **Decoding the Sequence**: If a decoder were to process this sequence naively, it would interpret `c0 80` as an attempt to encode the null character (U+0000).\n",
    "\n",
    "4.  **The Violation**: The null character already has a valid, shorter, single-byte representation: `b'\\x00'`. Because a shorter representation exists, the two-byte version `b'\\xc0\\x80'` is classified as an illegal \"overlong\" encoding. Any compliant UTF-8 decoder will reject this sequence as invalid.\n",
    "\n",
    "In short, any two-byte UTF-8 sequence starting with `0xc0` or `0xc1` is invalid for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f839bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116, 104, 101]\n"
     ]
    }
   ],
   "source": [
    "print(list(b'the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a437d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<regex.Match object; span=(0, 4), match='some'>\n",
      "<regex.Match object; span=(4, 9), match=' text'>\n",
      "<regex.Match object; span=(9, 14), match=' that'>\n",
      "<regex.Match object; span=(14, 16), match=' i'>\n",
      "<regex.Match object; span=(16, 19), match=\"'ll\">\n",
      "<regex.Match object; span=(19, 23), match=' pre'>\n",
      "<regex.Match object; span=(23, 24), match='-'>\n",
      "<regex.Match object; span=(24, 32), match='tokenize'>\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "# (used by GPT-2; Radford et al., 2019) from github.com/openai/tiktoken/pull/234/files:\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "for item in re.finditer(PAT, \"some text that i'll pre-tokenize\"):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40794f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677493c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
